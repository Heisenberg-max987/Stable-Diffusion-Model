{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXIvswhikCl8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder will reduce the dimention/size of the image(data), but increase the features of each pixels of images. We keep reducing the size of each images but keep incerease the features of each pixels of image.this means each pixels will capture more data about the area it belongs. Each pixel will be represented by the 3 channels of RGB. During this process number of pixelis diminishing but each pixel capturing more information.\n",
        "\n",
        "the job of encoder and decoder of vae is to encode and image or noise to a compressed version of image or noise. So that then we can take the latent and run it to the unet and then after the last step of the dinoissification we take this compressed version or latent and we pass it to the decoder and get the output image."
      ],
      "metadata": {
        "id": "4J6LKp57kH1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imoorting libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from decoder import VAE_AttentionBock, VAE_ResidualBock\n",
        "\n",
        "\n",
        "\n",
        "class VAE_Encoder(nn.Sequential):\n",
        "#this will inheret from sequential module. basically our encoder is a sequence of super modules.\n",
        "#It's actually a sequence of super modules, each module reduce the dimention size but increase the features of the data\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            #(batch_size,channels, height, width)=>(batch_size, 128, height, width)\n",
        "            nn.conv2D(3, 128, kernel_size=3,padding=1),\n",
        "#(initially we will have three channels and we convet into 128 channels, with kernel size 3 and padding 1)we start it as batch size define it as 3 channels, image will have a height, width of 512*512 . The convolution will convert into 128 with the same heigt and width, why in this case the height and width didnt change? =>padding\n",
        "\n",
        "            #(Batch size, 128, height, weight)=>(batch_size, 128, height, width)\n",
        "            VAE_ResidulBlock(128,128), #this residual block in a combination of bunch of convolution and nomaliation), (128,128)=>how many input channels and output channel ,in residual block we dint change the size of the image)\n",
        "            #(Batch size, 128, height, weight)=>(batch_size, 128, height, width)\n",
        "            VAE_ResidulBlock(128,128),\n",
        "            #(Batch size, 128, height, weight)=>(batch_size, 128, height/2, width/2)\n",
        "            nn.Conv2D(128,128, kernel_size=3, stride=2, padding=0),#the input channels of the conv is 128 because the output of the last block is 128, so now the size of the image is half of previous size, height width reduced because of stride=2, )\n",
        "            #(Batch size, 128, height/2, weight/2)=>(batch_size, 256, height/2, width/2)\n",
        "            VAE_ResidulBlock(128,256), # we just incerese the number of features , didnt increase the size)\n",
        "            #(Batch size, 128, height/2, weight/2)=>(batch_size, 256, height/2, width/2)\n",
        "            VAE_ResidulBlock(256,256),\n",
        "            #(Batch size, 256, height/2, weight/2)=>(batch_size, 256, height/4, width/4)\n",
        "            nn.Conv2D(256,256, kernel_size=3, stride=2, padding=0),\n",
        "            #(Batch size, 256, height/4, weight/4)=>(batch_size, 256, height/4, width/4)\n",
        "            VAE_ResidulBlock(256,512),\n",
        "            #(Batch size, 256, height/4, weight/4)=>(batch_size, 256, height/4, width/4)\n",
        "            VAE_ResidulBlock(512,512),\n",
        "            #(Batch size, 256, height/4, weight/2)=>(batch_size, 256, height/8, width/8)\n",
        "            nn.Conv2D(512,512, kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "            VAE_ResidulBlock(512,512),\n",
        "\n",
        "            VAE_ResidulBlock(512,512),\n",
        "            #(Batch size, 512, height/8, weight/8)=>(batch_size, 256, height/8, width/8)\n",
        "            VAE_ResidulBlock(512,512),\n",
        "\n",
        "            VAE_AttentionBlock(512,512), # this attention block makes all pixel relate to each others, each pixel will relate to each other globally while conv relate pixels locally whose are close to the corresponding pixel)\n",
        "            #(Batch size, 512, height/8, weight/8)=>(batch_size, 256, height/8, width/8)\n",
        "            VAE_ResidulBlock(512,512),\n",
        "            #(Batch size, 512, height/8, weight/8)=>(batch_size, 256, height/8, width/8\n",
        "            nn.GroupNorm(32,512),\n",
        "\n",
        "            nn.SiLU(),# practically this activation function works better so they used this\n",
        "            #(Batch size, 512, height/8, weight/8)=>(batch_size, 8, height/8, width/8\n",
        "            nn.Conv2D(512,8, kernel_size=3, padding=1),\n",
        "            #(Batch size, 8, height/8, weight/8)=>(batch_size, 8, height/8, width/8\n",
        "            nn.Conv2D(8,8, kernel_size=1,padding=0), #size doesnt change because kernel size"
      ],
      "metadata": {
        "id": "OO0wf3ikkwoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in decoder , we will reduce the number of channels and increase the size of the image.\n",
        "here we need to build three blocks, attention block , residual block and decoder block, we build attention block in a sperate class"
      ],
      "metadata": {
        "id": "tS-9hKLdaYhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from attention import SelfAttention\n",
        "\n",
        "#to define attention we need to define self attention block, we did it after residual block\n",
        "class VAE_AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels:int):\n",
        "        super().__init__()\n",
        "        self.groupnorm=nn.GroupNorm(32,channels)#channels is always 32 in stable diffusion\n",
        "        self.attention=SelfAttention(1, channels) # attention block has a group normalization and also a attention which is self attention\n",
        "\n",
        "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
        "        #x: (Batch_size, features, Height, Width)\n",
        "\n",
        "        #again we make a residual connection\n",
        "        residue=x\n",
        "\n",
        "        #first thing we do is we extract the shape\n",
        "        n,c,h,w=x.shape #n=batch size, c= number of channels, height, width\n",
        "\n",
        "\n",
        "        #then we will self attention among all the pixels of this image\n",
        "        #(batch_size, featres, height, width)->(batch_size, features, h*w)\n",
        "        x=x.view(n,c,h*w) #now we have a sequence, where each item represents a pixel bcz we do h*w\n",
        "\n",
        "        #then we transpose it like this\n",
        "        #(batch_size, featres, height*width)->(batch_size,h*w, features)\n",
        "        x=x.transpose(-1,-2)\n",
        "        #then we do attention\n",
        "        x=self.attention(x)\n",
        "\n",
        "        #(batch_size, height*width, features)->(batch_size,features, h*w)\n",
        "        x=x.transpose(-1,-2)#then we do inverse transformation, because we transpose before to do attention now we transpose back\n",
        "\n",
        "        #then we again remove the multiplication\n",
        "        #we go from here (batch_size, features, h*w)->(batch_size, features, height, width)\n",
        "        x=x.view(n,c,h,w)\n",
        "\n",
        "        #then we add the residual connection\n",
        "        x+residue #residual connection wil not change the size of input\n",
        "        #and we return x\n",
        "        return x\n",
        "\n",
        "        return x+residue\n",
        "\n",
        "class VAE_ResidualBock(nn.module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.groupnorm_1=nn.GroupNorm(32,in_channels)\n",
        "        self.conv_1=nn.Conv2D(in_channels,in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "\n",
        "        self.groupnorm_2=nn.GroupNorm(32,in_channels)\n",
        "        self.conv_2=nn.Conv2D(in_channels,in_channels, kernel_size=3, padding=1)\n",
        "#skip connection part, skip connection basically means u take input ,skip some layers, n then u connect it with output of last layer)\n",
        "#we also need this residual connection if the two channels are different we need to create another intermediate layer)\n",
        "        if in_channels==out_channels:\n",
        "            self.residual=nn.Identity()\n",
        "        else:\n",
        "            self.residual=nn.Conv2D(in_channels, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
        "         #x:(Batch_size, In_channels,Height, Width)\n",
        "\n",
        "         residue=x\n",
        "         x=self.groupnorm_1(x)\n",
        "         x=F.silu(x)\n",
        "         x=self.conv_1(x)\n",
        "         x=self.groupnorm_2(x)\n",
        "         x=F.silu(x)\n",
        "         x=self.conv_2(x)\n",
        "         return x+self.residual_layer(residue) #if the number of output channels are not equal to input channels we can not do x+residue, because dimensions will not match\n",
        "         #so we create [self.residual=nn.Conv2D(in_channels, out_channels, kernel_size=1, padding=0)] layer to convert the input channel to the output channel of x such that\n",
        "         #sum can be done ,so we apply (residual_layer of rasidue) instead of residue\n",
        "\n"
      ],
      "metadata": {
        "id": "V8VTTp1xanZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we built attention block before now we built self attention, there are two types of attention in stable diffusion e.g. self and cross attention, we now built it in different class"
      ],
      "metadata": {
        "id": "53Ob4He0n-lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import M\n",
        "from _typeshed import Self\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, n_heads:int, d_embed:int, in_proj_bias=True, out_proj_bias=True): #we have the number of head and embeding(token in transformer model we use in pixel of images, we have also the bias for w matrices),\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        #lets define the w matrices w_q,w_k,w_v\n",
        "        self.in_proj=nn.Linear(d_embed, 3*d_embed, bias=in_proj_bias) #we built a big linear layer instead of build them sepertely in_proj_bias means in projection bias\n",
        "\n",
        "        self.out_proj=nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "         #then we set nmber of heads\n",
        "        self.n_heads=n_heads\n",
        "        #then we set dimension of each head, d_head means basically if we have multihead, each head will watch a part of the embading of each token\n",
        "        self.d_head=d_embed//n_heads\n",
        "\n",
        "        #lets implement the forward\n",
        "        def forward(self,x:torch.Tensor, casual_mask=False)->torch.Tensor: #we can also apply a mask, mask is the way to avoid letting token that comes after it, but only with the token that comes before it\n",
        "        #x:(Batch_size, Seq_Len,Dim)\n",
        "\n",
        "        #first we extract the shape\n",
        "        input_shape=x.shape\n",
        "        batch_size, sequence_length, d_embed=input_shape\n",
        "\n",
        "        #then we convert it into another shape\n",
        "        #(Batch_size, Seq_Len, Dim)->(Batch_size, Seq_Len, 3*Dim)\n",
        "        intermim_shape=(batch_size, sequence_length, self.n_heads, self.d_head)\n",
        "\n",
        "        #then we apply the q,k,v\n",
        "        #(Batch_size, seq_len, Dim) -> (Batch_size, seq_len,Dim*3)->3 tensors of shape(Batch_size, Seq_len,Dim)\n",
        "        q, k, v=self.in_proj(x).chunk(3,dim=-1) #\n",
        "\n",
        "\n",
        "        #now we can split the q,k,v according to the number of heads\n",
        "        #(Batch_size, Seq_Len, Dim)->(Batch_size, Seq_Len, n_heads,H, Dim/H)->(Batch_size,H, Seq_Len, Dim/H) (due to transpose)\n",
        "        #(Batch_size, Seq_Len, n_heads,H, Dim/H)means it head will watch the full sequence but only a part of embeding of each token (in this case pixel)\n",
        "        q=q.view(intermim_shape).transpose(1,2)\n",
        "        k=k.view(intermim_shape).transpose(1,2)\n",
        "        v=v.view(intermim_shape).transpose(1,2)\n",
        "\n",
        "        #then we calculate the attention according to formula\n",
        "\n",
        "        weight=q@k.transpose(-1,-2)\n",
        "        if casual_mask:\n",
        "            #mask where the upper tringle(above the principal diagonal) is made up of 1\n",
        "            mask=torch.ones_like(weight,dtype=torch.bool).triu(1)\n",
        "            weight.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        weight=weight/math.sqrt(self.d_head)\n",
        "        #then we apply softmax\n",
        "        weight=F.softmax(weight, dim=-1)\n",
        "\n",
        "\n",
        "        output=weight@v\n",
        "        output = output.transpose(1, 2)\n",
        "        output = output.reshape(input_shape)\n",
        "        output = self.out_proj(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "B9QMz2JBoa_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}